# Description
refernce by [kaggle](https://www.kaggle.com/CVxTz/audio-data-augmentation/notebook) , [paper](https://www.researchgate.net/publication/316604394_Using_Deep_Gated_RNN_with_a_Convolutional_Front_End_for_EndtoEnd_Classification_of_Heart_Sound) and other idea 
we divide the heart sound augmentation into two major categories, 
  * <b>signal aspect</b>: feature is generated by programming
  * <b>knowledge aspect</b>: feature is <font color=red>not</font> generated by programming
    
each category have correspoding detail augmenation method, please check the following list

# Summary
* signal aspect
  - [x] 1.<font color=blue>[kaggle]</font> white noise
  - [ ] 2.<font color=green>[paper]</font> Dither <font color=gray>white noise imply this behavior</font>
  - [x] 3.<font color=blue>[kaggle]</font> Stretch (± second)
  - [ ] 4.<font color=blue>[kaggle]</font> Frequency (± hz) <font color=gray>stretch imply frequency adjust</font>
  - [ ] 5.<font color=green>[paper]</font> Pitch (± semitone) <font color=gray>period observation not clear in heart sound</font>
  - [ ] 6.<font color=blue>[kaggle]</font> Shift <font color=gray> i think its useless for model training</font>
  - [x] 7.Silence
  - [x] 8.<font color=blue>[kaggle]</font> Volume
* knowledge aspect (mix some ohter voice)
  - [x] 1.<font color=green>[paper]</font> mix humand speech ([dataset](http://www.voiptroubleshooter.com/open_speech/american.html))
  - [ ] 2.mix environment voice such as office 
    - [ ] office voice <font color=gray>(hard to get rich variance)</font>
    - [ ] home voice <font color=gray>(hard to get rich variance)</font>
  - [x] 3.mix breath (lung) voice

# Audio example for each augmentation method
we use the following file as based line wave, which contain 8 second normal heart beat 
```python
physionet/training/training-b/b0038.wav
```
this wave will combine diffent augmenation methods into a new wave<br>

<audio src="https://drive.google.com/uc?export=view&id=1HPypU4qu6cCFVgnX0oHnPGtpA54JaE4Y" controls preload></audio>


![orignal_jpg](https://drive.google.com/uc?id=1y3H6R-0xpZ1KcmaFYnpbvMdQRiHBA_-I)

## Signal aspect - 1.white noise

white noise is a random signal having equal intensity at different frequencies <br>
after merge b0038.wav and white noise, we can get this wave file
<audio src="https://drive.google.com/uc?id=1GnEkJwZ0JYn9mCIV4GJEskVsDmvjermh" controls preload> </audio>

![orignal_and_white_noise](https://drive.google.com/uc?id=1WH__3N7sdJgFz-BwMUuMvL3fgrdPNMq6)

## Signal aspect - 3 Stretch up/down
we could use python librosa package to implement stretch augmentation<br>
(sample code: https://www.kaggle.com/CVxTz/audio-data-augmentation/notebook)<br>
```python
data = librosa.effects.time_stretch(data, rate)
```
the following figure show the output after stretch up 25% (b0038.wav 8s -> 10s)<br>
<audio src="https://drive.google.com/uc?id=1dClsgfjytxpNdkB1fFKEXYTtUk7ynanX" controls preload></audio>

* stretch will effect duration of heart beat interval <br>
comparing b0038.wav and b0038_stretch case, <br>
in b0038.wav, the distance between first S2 and second S2 have totally 661 sample <br>
in b0038_stretch.wav the distance between first S2 and second S2 have totally 826 sample <br>
826 = 661 * 1.25 <br>
it means, stetch is linear-interpolation operation
* spectrum(frequency) will be changed <br>
in original b0038.wav, S2-S2 distance is 661 samples, almost 1.29 frame <br>
in stretch b0038.wav, S2-S2 distance is 826 samples, almost 1.61 frame <br>
spectrum analysis is used fixed frame size FFT (such as 512 sample) <br>
when frame size is fixed, we will get differen spectrum output <br>

![stretch up 25%](https://drive.google.com/uc?id=1RE2CgLPfQvfAZxME69oAhnM_LQM-2NJl)

## Signal aspect - 7 Silence
we could randomly silence some part of hear sound wav <br>
in the following figure, new b0038.wav contain two silence(0.4s and o.5s) <br>
[b0038_with_silence](https://drive.google.com/open?id=10FxbPrhnzJMVS2wnUjZVe56cLKzQD9wa)
<audio src="https://drive.google.com/uc?id=1xRB_shg9nFjWeWSk28BOC6FVvZ_LJ53z" controls preload></audio>

## Signal aspect -8 Volume
it is easily to image the wave sound after change volume<br>
the following figure show that we adjust b0038.wave to -10db <br>
[b0038 minus 10db](https://drive.google.com/open?id=1a4wcdv9PIi7OwSR3BvN9_EMA9EkYwWFa)
![b0038_minus10db](https://drive.google.com/uc?id=1eIYew6bI2YB39AWZA5ZNiuX0BaeS72u0)

## Knowledge aspect - 1 Mix human sound

sometime, we mighe recod other human sound if we use stethoscope to measure or recoding hear sound. <br>
in the following public  [dataset](http://www.voiptroubleshooter.com/open_speech/american.html)<br>
it contain 60 wav file, each file contain 10 sentence, totally 600 sentence)<br>
* American English (25)
* British English (15)
* Mandarin Chinese (4)
* French (6)
* India (10) <br>

in the following figure, we mix b0038.wav and OSR_us000_0010_8k.wav<br> (one of American English dataset from Harvard Sentences)<br>
the content of choised American English is<br>
```python
The birch canoe slid on the smooth planks.
Glue the sheet to the dark blue background.
```
before we merge wave, we adjust -20db for human sound (OSR_us000_0010_8k.wav) <br>
we sill can hear and understand what she say <br>
[b0038 mix human sound](https://drive.google.com/open?id=1cE0iFZLo3qNJVMZhalqaspLDw_HSu59P)

![mix_human_sound](https://drive.google.com/uc?id=1NsTwyOXsph5c9lu3MBACVL3K7Gb2Uepz)

## Knowledge aspect -2 Mix environment sound                                                                                  
we assume stethoscope will be used in following environments
* office - include talk, telphone ring, keybord typing, etc ...
* home - include family talk, TV, music, etc ...

** I could not find suitable public database for this purpuse

## Knowledge aspect -3 Mix breath sound (lung)
we don't find public dataset for breath sound, but I think it is important issue on data collection)<br>
in the following figure,
* 1th row indicate [b0038 wav](https://drive.google.com/uc?export=view&id=1HPypU4qu6cCFVgnX0oHnPGtpA54JaE4Y)
* 2th row indicate the breath sound [[wav link](https://drive.google.com/open?id=1Whz6jHDKvgB42c4WY4eOa0QW-Q8yx4zH)], post process w/ -10 db amplify adjustment <br>(extracted from youtube: https://www.youtube.com/watch?v=z4Fu1udzrTw&index=4&list=PLLKSXV1ibO86qgE2y9cMqNFmh6LfOa8RM)
* 3th row indicate the wav[[b0038 mix breath sound](https://drive.google.com/open?id=1MinvNHq8pEfkszsX6T5HH7xRKXtl9fMF)] after mix b0038.wav and breath sound


![b0038 mix breath](https://drive.google.com/uc?id=1TO-gtjEFUMDIoDigzSsFW3f3vsfdkFBk)

```python

```
